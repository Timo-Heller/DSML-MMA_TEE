{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22c09070",
   "metadata": {},
   "source": [
    "#### PLEASE NOTE\n",
    "As this file contains lots of graphics, the file sizes exceeds more than 150MB.\n",
    "\n",
    "So this file is only prioviding the code without any outputs. \n",
    "\n",
    "Please calculate the outputs on your own: `Kernel --> Restart & Run All`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0693915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import date, time, datetime, timedelta\n",
    "\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "import math\n",
    "\n",
    "import random\n",
    "\n",
    "# Color list\n",
    "import matplotlib.colors as col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bbed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_csv(\"boston_2016.csv\")\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74d48a",
   "metadata": {},
   "source": [
    "## Visualizing the location of the stations to get more insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebd2e8c",
   "metadata": {},
   "source": [
    "### First Step: Get coordinates of stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501fa7a",
   "metadata": {},
   "source": [
    "As you can see at the open dataset of BlueBikes [here](https://s3.amazonaws.com/hubway-data/index.html), BlueBikes (originally Hubway) has done a relaunch in March 2018. Because of this, there were several data sets avaliable which contain the coordinates of stations. **However, no dataset did cover all stations we have in our provided dataset by the lecturer**. Thus, we need first to merge two potential files together to get a broader dataset for the `station <-> coordinates` mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cac281",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_location_1 = pd.read_csv(\"previous_Hubway_Stations_as_of_July_2017.csv\")\n",
    "geo_location_2 = pd.read_csv(\"Hubway_Stations_2011_2016.csv\")\n",
    "\n",
    "def dataFrameContainsEntry(df, name):\n",
    "    \"\"\"This method returns false, if the given name does never appear in the given dataset.\"\"\"\n",
    "    return len(df[df[\"Station\"]==name]) > 0\n",
    "\n",
    "#check whether geo_location_1 already contains some names\n",
    "for index, row in geo_location_2.iterrows():\n",
    "    if dataFrameContainsEntry(geo_location_1, row[\"Station\"]):\n",
    "        geo_location_2.drop(index=index, inplace=True)\n",
    "\n",
    "# Adding both datasets into one dataframe and dropping unnecessary values\n",
    "geo_location = pd.concat([geo_location_1, geo_location_2])\n",
    "geo_location.drop(labels=[\"Station ID\", \"Municipality\", \"publiclyExposed\", \"# of Docks\"], axis=1, inplace=True)\n",
    "geo_location.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e314c6",
   "metadata": {},
   "source": [
    "### Second Step: Map coordinates to `boston` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca15a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge():\n",
    "    #merge for start_station\n",
    "    merged_df = pd.merge(boston, geo_location, how=\"left\", left_on=\"start_station_name\", right_on=\"Station\")\n",
    "    merged_df.drop(labels=\"Station\", axis=1, inplace=True)\n",
    "    merged_df.rename(columns={'Latitude':'Latitude_Start', 'Longitude': 'Longitude_Start'}, inplace=True)\n",
    "    \n",
    "    #merge for end_station\n",
    "    merged_df = pd.merge(merged_df, geo_location, how=\"left\", left_on=\"end_station_name\", right_on=\"Station\")\n",
    "    merged_df.drop(labels=\"Station\", axis=1, inplace=True)\n",
    "    merged_df.rename(columns={'Latitude':'Latitude_End', 'Longitude': 'Longitude_End'}, inplace=True)\n",
    "    return merged_df\n",
    "\n",
    "    \n",
    "merged_df = merge()\n",
    "merged_df.head(5)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cf8d0",
   "metadata": {},
   "source": [
    "### Third Step: Check new dataset for any invalid entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b29b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_stations = merged_df[merged_df.isnull().any(1)]\n",
    "missing_stations.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29876f78",
   "metadata": {},
   "source": [
    "Alright, there are some stations that did not get any coordinates yet. Ivestigating further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a469780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMissingStartStations():\n",
    "    missing_stations = merged_df[merged_df[\"Latitude_Start\"].isnull()] #return any row where some column has NaN value\n",
    "    missing_stations_grouped = missing_stations.groupby(\"start_station_name\").nunique()\n",
    "    return missing_stations_grouped\n",
    "\n",
    "def getMissingEndStations():\n",
    "    missing_stations = merged_df[merged_df[\"Latitude_End\"].isnull()] #return any row where some column has NaN value\n",
    "    missing_stations_grouped = missing_stations.groupby(\"end_station_name\").nunique()\n",
    "    return missing_stations_grouped\n",
    "\n",
    "def getMissingStations():\n",
    "    missing_stations_grouped_start = getMissingStartStations()\n",
    "    missing_stations_grouped_end = getMissingEndStations()\n",
    "\n",
    "    missing_stations_grouped_start.reset_index(inplace=True)\n",
    "    missing_stations_grouped_start.drop(columns=[\"end_time\", \"start_station_id\", \"end_station_id\", \n",
    "                                                 \"end_station_name\", \"bike_id\", \"user_type\", \n",
    "                                                 \"Latitude_Start\", \"Longitude_Start\", \"Latitude_End\", \"Longitude_End\"], inplace=True)\n",
    "\n",
    "    missing_stations_grouped_end.reset_index(inplace=True)\n",
    "    missing_stations_grouped_end.drop(columns=[\"start_time\", \"start_station_id\", \"end_station_id\", \n",
    "                                               \"start_station_name\", \"bike_id\", \"user_type\", \n",
    "                                               \"Latitude_Start\", \"Longitude_Start\", \"Latitude_End\", \"Longitude_End\"], inplace=True)\n",
    "\n",
    "    missing_stations_total = pd.merge(missing_stations_grouped_start, missing_stations_grouped_end, how='outer', left_on=\"start_station_name\", right_on=\"end_station_name\")\n",
    "    missing_stations_total.drop(columns=\"end_station_name\", inplace=True)\n",
    "    missing_stations_total.rename(columns={'start_station_name': 'station_name', \n",
    "                                           'start_time': 'Count of trips started at this station', \n",
    "                                           'end_time': 'Count of trips ended at this station'}, inplace=True)\n",
    "    return missing_stations_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f33356",
   "metadata": {},
   "outputs": [],
   "source": [
    "getMissingStations()\n",
    "#Display stations which have NaN values and how often these stations were used in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2ee31",
   "metadata": {},
   "source": [
    "**Argh, although we already have used two different datasets, there are still 9 stations without coordinates. Most of them are even quite frequently used, so we cannot just drop them (e. g. `TD Garden - Causeway at Portal Park #1`)**. Need to investigate manually using more datasets provided by BlueBikes [here](https://s3.amazonaws.com/hubway-data/index.html). <br>\n",
    "\n",
    "We used the following process:\n",
    "1. As the [datasets provided by BlueBikes](https://s3.amazonaws.com/hubway-data/index.html) contain trip data for every month **WITH right coordinates**, we look up in which month the station occur in the dataset (use the method `getStartTime` below). We only focus here on missing `start_stations` because as shown above in the table the missing end and start stations does not differ.\n",
    "1. Then go to [BlueBikes](https://s3.amazonaws.com/hubway-data/index.html) and download the dataset for the specific month.\n",
    "1. Then look up in the dataset for the station which coordinates are provided.\n",
    "1. Then create a new dataframe `geo_location_3` containing the missing coordinates.\n",
    "1. Finally, add these new dataframe `geo_location_3` to the dataframe `geo_location`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e35425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStartTime(station_name):\n",
    "    df = boston[boston[\"start_station_name\"]==station_name]\n",
    "    return station_name + \": \" + df[\"start_time\"].head(1).values #return first value found\n",
    "    \n",
    "for missing_station in getMissingStations()[\"station_name\"]:    \n",
    "    print(getStartTime(missing_station))\n",
    "    \n",
    "# Getting for every missing station the first start_time where the station does occur in the dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These values are taken manually from the BlueBikes website\n",
    "dict_1 = {'Station': \"18 Dorrance Warehouse\", 'Latitude': 42.387151, 'Longitude': -71.075978}\n",
    "dict_2 = {'Station': \"TD Garden - Causeway at Portal Park #1\", 'Latitude': 42.365942, 'Longitude': -71.060515}\n",
    " # This is no copy-error:\n",
    " # \"TD Garden - Causeway at Portal Park #1\" and \n",
    " # \"Upham's Corner - Ramsey St at Dudley St\" are located at the same place!\n",
    "dict_3 = {'Station': \"Upham's Corner - Ramsey St at Dudley St\", 'Latitude': 42.365942, 'Longitude': -71.060515}\n",
    "dict_4 = {'Station': \"Beacon St at Arlington St\", 'Latitude': 42.35554932336134, 'Longitude': -71.07284188270569}\n",
    "dict_5 = {'Station': \"Boylston St at Washington St\", 'Latitude': 42.352409, 'Longitude': -71.062679}\n",
    "dict_6 = {'Station': \"Central Square - East Boston\", 'Latitude': 42.373312125824704, 'Longitude': -71.0410200806291}\n",
    "dict_7 = {'Station': \"Commonwealth Ave at Buick St\", 'Latitude': 42.35062177153799, 'Longitude': -71.11288218766276}\n",
    "\n",
    "geo_location_3 = pd.DataFrame([dict_1, dict_2, dict_3, dict_4, dict_5, dict_6, dict_7])\n",
    "geo_location_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_location = pd.concat([geo_location, geo_location_3])\n",
    "geo_location.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb49d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merge()\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "getMissingStations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a386bab4",
   "metadata": {},
   "source": [
    "**Alright, we've almost got it, only these strange stations with names `8D OPS 01` and `8D OPS 03` left.**\n",
    "<br> These stations have cryptic names and also clearly-incorrect coordinates (e. g. (0,0) for ``8D OPS 01``). There were no metadata explaining these stations at BlueBike. Let's look at every entry containing one of the station:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkName(name: str) -> bool:\n",
    "    return name[:6]  == \"8D OPS\" #check if first 6 characters match the generic prefix \"8D OPS\"\n",
    "\n",
    "cryptic_stations = merged_df[(merged_df[\"start_station_name\"].apply(lambda x: checkName(x))) | \n",
    "                             (merged_df[\"end_station_name\"].apply(lambda x: checkName(x)))]\n",
    "cryptic_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa01f460",
   "metadata": {},
   "source": [
    "It seems that these stations are **not public rental stations** and that these \"trips\" refect inventory movements or other internal events not related to customer. This also seems rational as they only were used 2 times ``8D OPS 01`` and 4 times for ``8D OPS 03`` as you can see above and also never started or ended at a \"real\" station. Thus, we drop every row in the original dataframe which has one of these names in their `start_station_name` or `end_station_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(index=cryptic_stations.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_stations = merged_df[merged_df.isnull().any(1)]\n",
    "missing_stations_grouped = missing_stations.groupby(\"start_station_name\").nunique()\n",
    "missing_stations_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e8d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_stations_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50550d",
   "metadata": {},
   "source": [
    "##### Yeah, now every station has it's own coordinates! ðŸŽ‰ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea448b",
   "metadata": {},
   "source": [
    "### Fourth Step: Visualize the geographical position of the stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5addb2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folium needs the coordinates in one variable, so putting the values together.\n",
    "\n",
    "# We're rounding the corrdinates to 4 digits in order to remove some inaccuracies while getting the data.\n",
    "merged_df[\"Coordinates_Start\"] = list(zip(merged_df[\"Latitude_Start\"].round(4),merged_df[\"Longitude_Start\"].round(4))) \n",
    "merged_df[\"Coordinates_End\"] = list(zip(merged_df[\"Latitude_End\"].round(4),merged_df[\"Longitude_End\"].round(4))) \n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50e91af",
   "metadata": {},
   "source": [
    "##### Stations that are frequently used to start a trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d18f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map = folium.Map(location=(42.36, -71.071), tiles='Stamen Toner',zoom_start=12, control_scale=True, max_zoom=20)\n",
    "\n",
    "# add heat map\n",
    "heat_map.add_child(plugins.HeatMap(merged_df[\"Coordinates_Start\"], radius=20))\n",
    "\n",
    "heat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63241dd0",
   "metadata": {},
   "source": [
    "##### Stations that are frequently used to end a trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map = folium.Map(location=(42.36, -71.071), tiles='Stamen Toner',zoom_start=12, control_scale=True, max_zoom=20)\n",
    "\n",
    "# add heat map\n",
    "heat_map.add_child(plugins.HeatMap(merged_df[\"Coordinates_End\"], radius=20))\n",
    "\n",
    "heat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116d1908",
   "metadata": {},
   "source": [
    "We see almost no differences, **but some stations** (see e. g. `Hardvard Street`) **at the outside of the city are more often used to rent a bike** (likely people how use the bike to travel back inside the city) **instead of returning a bike**.\n",
    "_(Note: You have to zoom into both maps, to see the difference)_ \n",
    "\n",
    "This could indicate that BlueBikes need to pay attention especially to these outside bike stations in order to initiate inventory movements or provide some incentives (like discounts) for the people who start a trip in the city and bring the bike back e. g. to the station next to `Hardvard Street`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5c029",
   "metadata": {},
   "source": [
    "##### Frequently used stations (no matter whether used as start OR end station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating a new list holding all coordinates of the trips\n",
    "def addCoordinatesToList(coordinates, list):\n",
    "    list.append(coordinates)\n",
    "\n",
    "list_with_start_and_end_coordinates = []\n",
    "merged_df[\"Coordinates_Start\"].apply(lambda x: addCoordinatesToList(x, list_with_start_and_end_coordinates))\n",
    "merged_df[\"Coordinates_End\"].apply(lambda x: addCoordinatesToList(x, list_with_start_and_end_coordinates))\n",
    "list_with_start_and_end_coordinates[0:5] #holds a list with all coordaintes of the data table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3788c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map = folium.Map(location=(42.36, -71.071), tiles='Stamen Toner',zoom_start=12, control_scale=True, max_zoom=20)\n",
    "\n",
    "# add heat map\n",
    "heat_map.add_child(plugins.HeatMap(list_with_start_and_end_coordinates, radius=20))\n",
    "\n",
    "heat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d7fcd",
   "metadata": {},
   "source": [
    "In conclusion, this map provides an total overview of the activity level of the different stations. This can be useful to determine stations, that are not used frequently and thus, could be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ee092",
   "metadata": {},
   "source": [
    "## KPI 1: Estimated Revenue "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343da690",
   "metadata": {},
   "source": [
    "Although, we only have roughly fitting data for calculating the revenue, we decided to keep track on it in order to be able to show one of the most important KPIs for business practionioners: **The renvenue**!\n",
    "\n",
    "As we have the dataset for 2016, we needed to do some more research on the pricing in 2016. We used the `Wayback Machine` in order to get the old website of BlueBikes (which were named at this time \"hubway\") and their pricings back in 2016. This revealed the following pricing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47901608",
   "metadata": {},
   "source": [
    "![Fees](pricing_membership_overtimefees_960px.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac96a2",
   "metadata": {},
   "source": [
    "_If the bike has not been returned and correctly docked at a station after 24 hours, the bike is considered stolen and a fee of $1,000 will be charged to your credit card._\n",
    "\n",
    "Source: [https://web.archive.org/web/20160331184858/http://www.thehubway.com/pricing](https://web.archive.org/web/20160331184858/http://www.thehubway.com/pricing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545992c",
   "metadata": {},
   "source": [
    "So the following equation holds:\n",
    "$$Total Revenue = Membership Fees + Overtime Fees $$\n",
    "\n",
    "As we do not have any data about active members, we unfortunately cannot calcualte the total revenue. **We instead focus on calculating the additional income generated by the `Overtime Fees`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ec0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston[\"start_time\"] = pd.to_datetime(boston[\"start_time\"])\n",
    "boston[\"end_time\"] = pd.to_datetime(boston[\"end_time\"])\n",
    "boston[\"trip_time\"] = boston[\"end_time\"] - boston[\"start_time\"]\n",
    "boston[\"day\"] = boston[\"start_time\"].apply(lambda ts: ts.date())\n",
    "boston[\"month\"] = boston[\"start_time\"].apply(lambda ts: ts.month)\n",
    "boston[\"hour\"] = boston[\"start_time\"].apply(lambda ts: ts.hour)\n",
    "boston.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12916153",
   "metadata": {},
   "source": [
    "As the dataset is quite huge and the algorithm has to calcualte the fee manually for each trip, it takes up to 5 minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0436af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAmountOfHalfHour(trip_time):\n",
    "    anz = trip_time / pd.Timedelta('30min')\n",
    "    return math.floor(anz) #round down: 1.5 becomes 1 as next full 1/2 hour not reached.\n",
    "\n",
    "def getOvertimeFeeForTrip(row):\n",
    "    trip_time = row[\"trip_time\"]\n",
    "    user_type = row[\"user_type\"]\n",
    "    \n",
    "    overtime_fee = 0\n",
    "    \n",
    "    if trip_time < pd.Timedelta('30min'):\n",
    "        overtime_fee = 0\n",
    "    elif trip_time < pd.Timedelta('60min'):\n",
    "        overtime_fee = 2\n",
    "    elif trip_time < pd.Timedelta('90min'):\n",
    "        overtime_fee = 4\n",
    "    else:\n",
    "        overtime_fee = 4\n",
    "        # every 1/2 hour +8$ are applied\n",
    "        count = getAmountOfHalfHour(trip_time)\n",
    "        count = count - 3 #first 90mins are already included by the 4$ above\n",
    "        overtime_fee += 8*count\n",
    "        \n",
    "    #applying 25% discount for members\n",
    "    if user_type == \"Subscriber\":\n",
    "        overtime_fee -= overtime_fee*0.25\n",
    "    \n",
    "    return overtime_fee\n",
    "        \n",
    "    \n",
    "boston[\"overtime_fee\"] = boston.apply(getOvertimeFeeForTrip, axis=1) #use axis=1 to pass the series object row-wise\n",
    "boston.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1957f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete outlier\n",
    "indexes_to_delete = boston[boston[\"overtime_fee\"] > 300].index\n",
    "boston.drop(index=indexes_to_delete, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa764b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_revenue_per_day = boston[[\"day\", \"overtime_fee\"]].groupby(\"day\").sum() # Getting the total revenue per day\n",
    "total_revenue_per_day.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954912ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "fig,ax = plt.subplots(figsize=(16,9)) \n",
    "\n",
    "ax.plot(total_revenue_per_day[\"overtime_fee\"])\n",
    "ax.set_title(\"Revenue through overtime fees per day\",fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed278431",
   "metadata": {},
   "source": [
    "We can see a **seasonal trend**: In summer months, people tend to use the bikes longer then in winter months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,9)) \n",
    "sns.boxplot(x=\"hour\",y=\"overtime_fee\",data=boston,palette=\"mako\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfd724",
   "metadata": {},
   "source": [
    "It seems, that most of the time the customer do not overrun their time greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4947988",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,9)) \n",
    "sns.distplot(boston[\"overtime_fee\"], kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aadb72d",
   "metadata": {},
   "source": [
    "Yeah, let's remove the outliers for this visualization and only look at the height of fee if any overtime fee was charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,9)) \n",
    "df = boston[(boston[\"overtime_fee\"] > 0 ) & (boston[\"overtime_fee\"] < 80)]\n",
    "sns.distplot(df[\"overtime_fee\"], kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaedce1",
   "metadata": {},
   "source": [
    "We see, that customers try most of the time to not exceed the inlcuded free 30mins. However, lot's of people exceed 30mins by a short amount of time and thus are charged by 1.5 US-Dollar or 2 US-Dollar (depending on whether they are customer or not). \n",
    "Only rarely, the time limit of 30mins gets more greatly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimePassedInSec(datetime):\n",
    "    return datetime.total_seconds()\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(16,9)) \n",
    "\n",
    "boston_new = boston.copy()\n",
    "boston_new[\"trip_time\"] = boston_new[\"trip_time\"].apply(lambda x: getTimePassedInSec(x))\n",
    "\n",
    "\n",
    "sns.scatterplot(ax=ax, x=\"trip_time\", y=\"overtime_fee\", \n",
    "                data=boston_new, hue=\"user_type\")\n",
    "\n",
    "ax.set_xlabel(\"Trip duration in seconds\")\n",
    "ax.set_ylabel(\"Overtime fees charged in $\")\n",
    "ax.set_title(\"Overtime fees per trip length\", fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e90fa",
   "metadata": {},
   "source": [
    "It seems that subscribers tend to do longer trips than customers (propably due to the 25% discount the subscribers get)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0142356",
   "metadata": {},
   "source": [
    "## KPI 2: Most Popular Stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a620b8f",
   "metadata": {},
   "source": [
    "## KPI 2: Popular trip routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1464500",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ids_per_day_group = boston[[\"day\", \"start_station_id\", \"start_time\"]].groupby([\"day\", \"start_station_id\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_ids_per_day_group = boston[[\"day\", \"end_station_id\", \"start_time\"]].groupby([\"day\", \"end_station_id\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ed7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_ids_per_day_group = start_ids_per_day_group + end_ids_per_day_group\n",
    "total_ids_per_day_group.rename(columns={'start_time':'Times this trip appears in dataset'}, inplace=True)\n",
    "total_ids_per_day_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_trips_group = boston.groupby([\"start_station_id\", \"end_station_id\"]).size().unstack(level=0)\n",
    "\n",
    "#popular_trips_group.head(500)\n",
    "\n",
    "#every NaN means that in whole 2016 there was no trip to it, so replace NaN with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf36a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in popular_trips_group.index:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1, 1, figsize=(20, 15))\n",
    "sns.heatmap(popular_trips_group, robust=True)\n",
    "plt.show()\n",
    "# TO DO beschrifte jede Linie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2797f1d",
   "metadata": {},
   "source": [
    "#### This heatmap looks a bit unordner, but it conveys quite useful information\n",
    "\n",
    "1. We see lots of dark point, but also some red points. These red and light points are especially important, as lots of trips start respective end there. That means that BlueBike has to take care that these stations are working correctly (e. g. no technical errors, always enough bikes available, might even add more docker stations for bikes) because otherwise a malfunction could directly lead to customer dissatification.\n",
    "1. Wee see that in general almost every station is used. That means that we do not have unnecessary stations that only create costs but no profit. However, some white spaces for the stations `211` or `214` could indicate that they are not often used. However, further investigation is needed.\n",
    "1. Stations `153` and `158` are huge outliners: They are only used by each other which is a quite unusual pattern (see the drawn through white line at the corresponding station IDs). Further investigation reveals that that are the technical 8D OPS stations that are not used publicly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe524dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most popular routes (used more than 1000 times)\n",
    "popular_trips_group = boston.groupby([\"start_station_id\", \"end_station_id\"]).size()\n",
    "popular_trips = pd.DataFrame(popular_trips_group[popular_trips_group > 1000], columns=[\"count\"])\n",
    "popular_trips.reset_index(inplace=True)\n",
    "popular_trips.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normalized dataset\n",
    "max = popular_trips[\"count\"].max()\n",
    "min = popular_trips[\"count\"].min()\n",
    "popular_trips[\"norm_value\"] = popular_trips[\"count\"].apply(lambda x: (x - min) / (max - min) )\n",
    "popular_trips.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bdf4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods for path visualization\n",
    "def getNameFromStationId(id):\n",
    "    df = merged_df[merged_df[\"start_station_id\"]==id]\n",
    "    name = df.iloc[0][\"start_station_name\"]\n",
    "    return name\n",
    "    \n",
    "    \n",
    "def getCoordinatesFromStationId(id):\n",
    "    df = merged_df[merged_df[\"start_station_id\"]==id]\n",
    "    \n",
    "    #as some routes overlap, we use a every small jitter in order to be able to display overlapping lines.\n",
    "    jitter = random.uniform(-0.0001,+0.0001)\n",
    "    lat = df.iloc[0][\"Latitude_Start\"] + jitter\n",
    "    long = df.iloc[0][\"Longitude_Start\"] + jitter\n",
    "    return [lat, long]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a702678",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_color_dict = col.TABLEAU_COLORS #use pre-defined colors\n",
    "\n",
    "colors = []\n",
    "for key, value in ordered_color_dict.items():\n",
    "   colors.append(value)\n",
    "\n",
    "\n",
    "map = folium.Map(location=(42.38, -71.1),  tiles='CartoDB positron', zoom_start=13, control_scale=True, max_zoom=20)\n",
    "\n",
    "# draw trajectory\n",
    "for index, (start_station_id, end_station_id, count, norm_value) in popular_trips.iterrows():\n",
    "    if start_station_id != end_station_id:\n",
    "        coordinates_list = []\n",
    "        coordinates_list.append(getCoordinatesFromStationId(start_station_id))\n",
    "        coordinates_list.append(getCoordinatesFromStationId(end_station_id))\n",
    "        displayMessage = \"Trip from station \" + getNameFromStationId(start_station_id) + \" to station \" + getNameFromStationId(end_station_id)\n",
    "        folium.PolyLine(coordinates_list, color=colors[index%len(colors)], popup=displayMessage, weight=20*norm_value, opacity=0.8).add_to(map)\n",
    "    else:\n",
    "        lat = getCoordinatesFromStationId(start_station_id)[0]\n",
    "        long = getCoordinatesFromStationId(start_station_id)[1]\n",
    "        folium.CircleMarker(location=(lat, long), popup=\"Trip started and ended at \" + getNameFromStationId(start_station_id), \n",
    "                            radius=20*norm_value, opcity=0.8, color=colors[index%len(colors)]).add_to(map)\n",
    "            \n",
    "    \n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb84828",
   "metadata": {},
   "source": [
    "We see a map with the most popular trips in 2016. Every trip which was used in 2016 more than 1000 times shown as a line on the map. To get the exact nameing of the start and end station, just click on any line. The more often a trip was used, the more thicker the line is. Colors do not have a special meaning, the are just used to differentiate the trips. Trips that started and ended at the same station, are marked as circle.\n",
    "\n",
    "**What insights do we obtain?**\n",
    "1. The most popular trips took place in the inner city. Bikes are escpecially used to travel over the `Massachusetts Ave` bridge.\n",
    "1. The stations in the inner city are used very often, so the decision of BlueBikes to build lots of stations in the inner city to enable the customers with a more flexible routing in the city was right.\n",
    "1. Outside the city, there are two places that are used very frequently, too and should be carefully monitored by BlueBikes: At `Davis Square` and `Financial district`.\n",
    "1. Two stations are **used frequently as start and end point**: `MIT at Mass Ave / Amherst St` and `The Esplanade - Beacon St. at Arlington St.`. Further investigation could determine whether people might miss another returning station and thus, are forced to return back to their start station as every other station would be too far away. This could indicate a potential lack of stations in the bike sharing infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5462ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
